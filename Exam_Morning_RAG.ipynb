{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf56921",
   "metadata": {},
   "source": [
    "# Exam (morning): Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7fb22f",
   "metadata": {},
   "source": [
    "### Personal Details (please complete)\n",
    "Double Click on Cell to edit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d35a25",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td>First Name: </td>\n",
    "    <td>Jaden</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Last Name:</td>\n",
    "    <td>Donati</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Student ID:</td>\n",
    "    <td>22582407</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Modul:</td>\n",
    "    <td>Machine Learning 2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Exam Date / Raum / Zeit:</td>\n",
    "    <td>20.05.2025 / Raum: SM O2.01  / 10:15 – 11:30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Erlaubte Hilfsmittel:</td>\n",
    "    <td>w.3ML2-WIN (Machine Leaning 2)<br>Open Book, Personal Computer, Internet Access</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>Not allowed:</td>\n",
    "  <td>The use of any form of generative AI (e.g., Copilot, ChatGPT) to assist in solving the exercise is not permitted. <br> However, using such tools as part of the exercise itself (e.g., making API calls to them if required by the task) is allowed. <br> Any form of communication or collaboration with other people is not permitted.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61847f62",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "### <b style=\"color: gray;\">(maximum achievable points: 48)</b>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Category</th>\n",
    "      <th>Description</th>\n",
    "      <th>Points Distribution</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Code not executable or results not meaningful</td>\n",
    "      <td>The code contains errors that prevent it from running (e.g., syntax errors) or produces results that do not fit the question.</td>\n",
    "      <td>0 points</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code executable, but with serious deficiencies</td>\n",
    "      <td>The code runs, but the results are incomplete due to major errors (e.g., fundamental errors when reading the data). Only minimal progress is evident.</td>\n",
    "      <td>25% of the maximum achievable points</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code executable, but with moderate deficiencies</td>\n",
    "      <td>The code runs and delivers partially correct results, but there are significant errors (e.g., the data types of the imported data do not meet the requirements of the question). The results are comprehensible but incomplete or inaccurate.</td>\n",
    "      <td>50% of the maximum achievable points</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code executable, but with minor deficiencies</td>\n",
    "      <td>The code runs and delivers a largely correct result, but minor errors (e.g., column name misspelled, timestamp not correctly formatted) affect the completeness of the result.</td>\n",
    "      <td>75% of the maximum achievable points</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code executable and correct</td>\n",
    "      <td>The code runs flawlessly and delivers the correct result without deficiencies.</td>\n",
    "      <td>100% of the maximum achievable points</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8df3dd",
   "metadata": {},
   "source": [
    "## Python Libraries und Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac9011e",
   "metadata": {},
   "source": [
    "## <b>Set Up (This part will <u>not</u> be evaluated!)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e253f40",
   "metadata": {},
   "source": [
    "#### <b>1.) Start a GitHub Codespaces instance based on your fork of this GitHub repository or open the notebook in Colab</b>\n",
    "#### <b>2.) Add API keys to either .env files for Codespaces or to the secrets for Colab</b>\n",
    "#### <b>3.) Please execute the two code cells below as soon as the Codespace/Colab has started and install the libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6e3474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/codespace/.python/current/lib/python3.12/site-packages (25.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.0.1\n",
      "    Uninstalling pip-25.0.1:\n",
      "      Successfully uninstalled pip-25.0.1\n",
      "Successfully installed pip-25.1.1\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.59 (from langchain-community)\n",
      "  Downloading langchain_core-0.3.60-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain<1.0.0,>=0.3.25 (from langchain-community)\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain-community)\n",
      "  Downloading sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-community) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain-community)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain-community)\n",
      "  Downloading langsmith-0.3.42-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-community) (2.2.4)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.59->langchain-community)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Downloading orjson-3.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain-community)\n",
      "  Downloading greenlet-3.2.2-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.60-py3-none-any.whl (437 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.42-py3-none-any.whl (360 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Downloading orjson-3.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (133 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading greenlet-3.2.2-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (603 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m603.9/603.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (245 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, tenacity, python-dotenv, pydantic-core, propcache, orjson, mypy-extensions, multidict, marshmallow, jsonpatch, httpx-sse, greenlet, frozenlist, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests-toolbelt, pydantic, aiosignal, pydantic-settings, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30/30\u001b[0m [langchain-community]ngchain-community]\n",
      "\u001b[1A\u001b[2KSuccessfully installed SQLAlchemy-2.0.41 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 annotated-types-0.7.0 dataclasses-json-0.6.7 frozenlist-1.6.0 greenlet-3.2.2 httpx-sse-0.4.0 jsonpatch-1.33 langchain-0.3.25 langchain-community-0.3.24 langchain-core-0.3.60 langchain-text-splitters-0.3.8 langsmith-0.3.42 marshmallow-3.26.1 multidict-6.4.4 mypy-extensions-1.1.0 orjson-3.10.18 propcache-0.3.1 pydantic-2.11.4 pydantic-core-2.33.2 pydantic-settings-2.9.1 python-dotenv-1.1.0 requests-toolbelt-1.0.0 tenacity-9.1.2 typing-inspect-0.9.0 typing-inspection-0.4.0 yarl-1.20.0 zstandard-0.23.0\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.11.0-cp312-cp312-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n",
      "Collecting groq\n",
      "  Downloading groq-0.25.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from groq) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
      "Downloading groq-0.25.0-py3-none-any.whl (129 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, groq\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [groq][32m1/2\u001b[0m [groq]\n",
      "\u001b[1A\u001b[2KSuccessfully installed distro-1.9.0 groq-0.25.0\n",
      "Collecting openai\n",
      "  Downloading openai-1.79.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Downloading openai-1.79.0-py3-none-any.whl (683 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, jiter, openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [openai]2m2/3\u001b[0m [openai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jiter-0.10.0 openai-1.79.0 tqdm-4.67.1\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (4.67.1)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (2.6.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.31.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.31.4-py3-none-any.whl (489 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [sentence-transformers]ence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed huggingface-hub-0.31.4 regex-2024.11.6 safetensors-0.5.3 sentence-transformers-4.1.0 tokenizers-0.21.1 transformers-4.51.3\n",
      "Requirement already satisfied: huggingface_hub[hf_xet] in /home/codespace/.python/current/lib/python3.12/site-packages (0.31.4)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.1 (from huggingface_hub[hf_xet])\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub[hf_xet]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub[hf_xet]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf-xet\n",
      "Successfully installed hf-xet-1.1.2\n",
      "Requirement already satisfied: faiss-cpu in /home/codespace/.python/current/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.169.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pydantic in /home/codespace/.python/current/lib/python3.12/site-packages (from google-generativeai) (2.11.4)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.12/site-packages (from google-generativeai) (4.12.2)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.25.0rc1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /home/codespace/.local/lib/python3.12/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/codespace/.local/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.4.0)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.25.0rc1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading google_api_python_client-2.169.0-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: uritemplate, pyasn1, protobuf, httplib2, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [google-generativeai]ogle-ai-generativelanguage]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cachetools-5.5.2 google-ai-generativelanguage-0.6.15 google-api-core-2.25.0rc1 google-api-python-client-2.169.0 google-auth-2.40.1 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.71.0 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --upgrade pip\n",
    "!pip install PyPDF2\n",
    "!pip install langchain-community\n",
    "!pip install faiss-cpu\n",
    "!pip install groq\n",
    "!pip install openai\n",
    "!pip install tqdm\n",
    "!pip install sentence-transformers\n",
    "!pip install huggingface_hub[hf_xet]\n",
    "!pip install faiss-cpu\n",
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a875319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import google.generativeai as genai\n",
    "from groq import Groq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4b53272",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_key = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c27b72",
   "metadata": {},
   "source": [
    "## <b>Tasks (This part will be evaluated!)</b>\n",
    "### Notes on the following tasks:\n",
    "\n",
    "In this part of the exam, you will build a Retrieval-Augmented Generation (RAG) pipeline that efficiently retrieves medical information from the package inserts of common medications. Imagine you are developing a system for pharmacists or medical professionals to quickly and accurately answer questions about medications. The following five package inserts are provided as your data source:\n",
    "\n",
    "- [data/Amoxicillin.pdf](data/Amoxicillin.pdf)\n",
    "- [data/bisoprolol.pdf](data/bisoprolol.pdf)\n",
    "- [data/citalopram.pdf](data/citalopram.pdf)\n",
    "- [data/metformin.pdf](data/metformin.pdf)\n",
    "- [data/paracetamol.pdf](data/paracetamol.pdf)\n",
    "\n",
    "Your task is to implement a RAG pipeline that retrieves relevant information from these package inserts and integrates it into the answer generation process. Use the provided instructions and your knowledge from the exercises.\n",
    "\n",
    "### Expected Results:\n",
    "\n",
    "1. Read in the provided package inserts and extract all text.\n",
    "2. Split the extracted text into manageable chunks using a text splitter (e.g., `RecursiveCharacterTextSplitter`).\n",
    "3. Create embeddings for the text chunks using a suitable model.\n",
    "4. Index the embeddings in a vector store (e.g., FAISS).\n",
    "5. Develop an appropriate prompt template.\n",
    "6. Build the RAG chain.\n",
    "7. Automatically generate a list of 10 test questions using a language model.\n",
    "8. Let your RAG pipeline answer the 10 generated questions.\n",
    "\n",
    "### Submission documents:\n",
    "\n",
    "Your submission should include:\n",
    "- The completed notebook (this file).\n",
    "- the vector store\n",
    "\n",
    "<b style=\"color:blue;\">Notes on the following tasks:</b>\n",
    "<ul style=\"color:blue;\">\n",
    "  <li>Pay attention to the specific details provided for each task.</li>\n",
    "  <li>Solve each task using Python code. Integrate your code into the code cells for each task.</li>\n",
    "  <li>Present your solution(s) as requested in each task.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033f466",
   "metadata": {},
   "source": [
    "#### <b>Task (1): Read all 5 PDFs from the 'data' folder and store their content for further use</b>\n",
    "<b>Task details:</b>\n",
    "- The files are located in the 'data' folder..\n",
    "- Display the length of the resulting string (number of characters).\n",
    "- Show the first 100 characters in the notebook output.\n",
    "<b style=\"color: gray;\">(max. points: 2)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdfcb555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Definiert ein Dateipfad-Muster, um alle PDF-Dateien im Ordner \"data\" zu finden.\n",
    "# 💡 Hinweis für die Prüfung: Du kannst diesen Pfad anpassen, wenn deine PDFs in einem anderen Verzeichnis liegen.\n",
    "glob_path = \"data/*.pdf\"\n",
    "\n",
    "# Initialisiert eine leere Zeichenkette, in der später der extrahierte Text gespeichert wird.\n",
    "text = \"\"\n",
    "\n",
    "# Iteriert über alle PDF-Dateien, die dem Pfad-Muster entsprechen.\n",
    "# tqdm zeigt einen Fortschrittsbalken an – hilfreich bei vielen Dateien.\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "\n",
    "    # Öffnet die aktuelle PDF-Datei im Lesemodus (\"rb\" = read binary).\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "\n",
    "        # Initialisiert den PDF-Reader für die geöffnete Datei.\n",
    "        reader = PdfReader(file)\n",
    "\n",
    "        # Extrahiert den Text aus jeder Seite der PDF, falls Text vorhanden ist,\n",
    "        # und hängt ihn an die `text`-Variable an (Seiten werden mit Leerzeichen verbunden).\n",
    "        text += \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07233c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the entire text: \n",
      "The first 100 characters of the text:\n"
     ]
    }
   ],
   "source": [
    "# Show the number of characters in the text\n",
    "print(f\"Number of characters in the entire text: \")\n",
    "\n",
    "# Show the first 100 characters of the text\n",
    "print(f\"The first 100 characters of the text:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d4887",
   "metadata": {},
   "source": [
    "#### <b>Task (2): Split the text into chunks appropriate for the task. Specify an overlap as well. Give a reason for your choice</b>\n",
    "<b>Task details:</b>\n",
    "- Use the data from the previous task.\n",
    "- Show the total number of chunks in the notebook.\n",
    "- Show the length of the first chunk in the notebook.\n",
    "- Explain you reasoning\n",
    "<b style=\"color: gray;\">(max. points: 4)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ea043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aad8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 🔧 Erstelle einen Text-Splitter mit folgenden Parametern:\n",
    "# - chunk_size: max. 2000 Zeichen pro Chunk\n",
    "# - chunk_overlap: jeweils 200 Zeichen Überlappung zwischen zwei Chunks\n",
    "#   → sorgt dafür, dass der Kontext beim Übergang zwischen Chunks nicht verloren geht\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,         # Maximale Länge eines Chunks in Zeichen\n",
    "    chunk_overlap=200        # Überlappung zwischen benachbarten Chunks\n",
    ")\n",
    "\n",
    "# ✂️ Teile den extrahierten PDF-Text mithilfe des Splitters in kleinere, überlappende Textabschnitte\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# 🧾 Jetzt enthält die Variable 'chunks' eine Liste von Textabschnitten, die jeweils max. 2000 Zeichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f348b15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 98\n",
      "Preview of the first chunk: Inhaltsverzeichnis\n",
      "Zusammensetzung\n",
      "Darreichungsform und Wirkstoffmenge pro Einheit\n",
      "Indikationen/Anwendungsmöglichkeiten\n",
      "Dosierung/Anwendung\n",
      "Kontraindikationen\n",
      "Warnhinweise und Vorsichtsmassnahmen\n",
      "Inte\n"
     ]
    }
   ],
   "source": [
    "# 📊 Zeige die Gesamtanzahl der erzeugten Text-Chunks an\n",
    "# Das ist hilfreich zur Kontrolle, wie viele Abschnitte aus dem PDF-Text entstanden sind\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "# 👁️‍🗨️ Zeige eine Vorschau auf den ersten Chunk (die ersten 200 Zeichen)\n",
    "# So kannst du kontrollieren, ob die Aufteilung sinnvoll funktioniert hat\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed18bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first chunk: <built-in method __sizeof__ of str object at 0x5f8239593190>\n"
     ]
    }
   ],
   "source": [
    "print(\"Preview of the first chunk:\", chunks[0].__sizeof__)\n",
    "#NOTIZ FÜR MICH Code eventuell noch löschen. Und Grund angeben für Chunk überlappung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the total number of chunks\n",
    "print(f\"Number of chunks: \")\n",
    "\n",
    "# Show the length of the first chunk\n",
    "print(f\"Length of the first chunk: \")\n",
    "\n",
    "#NOTIZ FÜR MICH LÄNGE NOCH ANGEBEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23affd5",
   "metadata": {},
   "source": [
    "##### Explanation (double click and add text):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3fec8",
   "metadata": {},
   "source": [
    "#### <b>Task (3): Initialize an embedding model</b>\n",
    "<b>Task details:</b>\n",
    "- Choose a suitable embedding model from Huggingface.\n",
    "- [Huggingface models](https://huggingface.co/spaces/mteb/leaderboard).\n",
    "- Consider the size of the model. It should be runnable in your Codespace.\n",
    "- Choose a model appropriate for the data.\n",
    "\n",
    "<b style=\"color: gray;\">(max. points: 2)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed99968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere den Namen des Embedding-Modells, das verwendet werden soll.\n",
    "# Dieses Modell wurde trainiert, um semantisch ähnliche Sätze in ähnliche Vektoren zu übersetzen.\n",
    "# \"multilingual\" bedeutet, dass es mit mehreren Sprachen (z. B. Englisch, Deutsch) umgehen kann.\n",
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "# Lade das ausgewählte Modell mit der SentenceTransformer-Bibliothek.\n",
    "# Es wird intern von HuggingFace geladen und kann sofort zur Vektorisierung verwendet werden.\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Erzeuge Embeddings (Vektoren) für alle Text-Chunks.\n",
    "# convert_to_numpy=True sorgt dafür, dass du ein NumPy-Array zurückbekommst (praktisch für spätere Verarbeitung).\n",
    "chunk_embeddings = model.encode(chunks, convert_to_numpy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35dd249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "# 📐 Ermittle die Anzahl der Dimensionen eines einzelnen Embedding-Vektors\n",
    "# chunk_embeddings ist ein 2D-Array mit der Form (Anzahl der Chunks, Anzahl der Dimensionen)\n",
    "# z. B. (120, 384) → 120 Chunks, jeder als Vektor mit 384 Werten\n",
    "\n",
    "d = chunk_embeddings.shape[1]  # Index [1] gibt die Spaltenanzahl = Vektor-Dimension\n",
    "\n",
    "# 🖨️ Gib die Dimension des Embeddings aus (wichtig für FAISS oder Ähnlichkeitsvergleiche)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b80dc61",
   "metadata": {},
   "source": [
    "#### <b>Task (4): Create a vector store</b>\n",
    "<b>Task details:</b>\n",
    "- Create a vector store\n",
    "- store the vector store (this is also helpful in case the codespace or colab needs a restart)\n",
    "<b style=\"color: gray;\">(max. achievable points: 6)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "077a187d-05be-4c30-a367-a4e1a19d4466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in FAISS index: 98\n"
     ]
    }
   ],
   "source": [
    "# 📦 Erstelle einen FAISS-Index zur schnellen Ähnlichkeitssuche\n",
    "# Wir verwenden hier \"IndexFlatL2\", der auf der euklidischen Distanz (L2) basiert.\n",
    "# Der Parameter `d` gibt die Anzahl der Dimensionen pro Vektor an (z. B. 384 bei deinem Modell).\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# ➕ Füge alle zuvor erzeugten Embedding-Vektoren in den Index ein\n",
    "# Dadurch kann FAISS später Anfragen (Queries) mit diesen vergleichen\n",
    "index.add(chunk_embeddings)\n",
    "\n",
    "# 🔢 Gib aus, wie viele Vektoren im Index gespeichert sind\n",
    "# Sollte gleich der Anzahl deiner Chunks sein\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d178ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 Speichere den FAISS-Index auf der Festplatte\n",
    "# → Damit musst du den Index beim nächsten Mal nicht neu berechnen\n",
    "#    (spart Zeit beim späteren Wiederverwenden)\n",
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "\n",
    "# 🗂️ Speichere zusätzlich die Text-Chunks als Mapping (Index → Originaltext)\n",
    "# → So kannst du später zu jedem Treffer die zugehörige Textpassage finden\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)  # Serialisiere und speichere die Liste der Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8aa85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa1d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e369261",
   "metadata": {},
   "source": [
    "#### <b>Task (5): Create a retriever function.</b>\n",
    "<b>Task details:</b>\n",
    "- Create a retriever function\n",
    "- Define the number of documents the retriever should return.\n",
    "- Test the retriever with the following query: `\"Welche Dosierung von Amoxicillin Axapharm wird für die Behandlung einer Endokarditis-Prophylaxe bei Erwachsenen empfohlen?\"`\n",
    "- If the retrieved chunks are not relevant, increase the number of chunks to be retrieved and repeat the query. \n",
    "- It does not have to be perfect; if nothing improves, continue with the current result.\n",
    "<b style=\"color: gray;\">(max. achievable points: 6)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ae6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_texts(query, k, index, chunks, model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Welche Dosierung +von Amoxicillin Axapharm wird für die Behandlung einer Endokarditis-Prophylaxe bei Erwachsenen empfohlen?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef402de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testen des retrievers\n",
    "retrieved_texts = \n",
    "\n",
    "print(retrieved_texts)\n",
    "print(len(retrieved_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd43ac",
   "metadata": {},
   "source": [
    "#### <b>Task (6): Implement a reusable RAG function and prompt template</b>\n",
    "<b>Task details:</b>\n",
    "- Write a function `get_answer_and_documents` that answers a question using your RAG pipeline.\n",
    "- The function should:\n",
    "  - Take as parameters: the question (`question`), the number of documents to retrieve (`k`), the FAISS index (`index`), and the list of text chunks (`chunks`).\n",
    "  - The prompt template should be tailored to the medical context, address medical professionals, and instruct the model to answer concisely and in German, using only the provided context. This is part of the task.\n",
    "  - Return both the answer and the retrieved documents.\n",
    "- Test the function with the question: `Ab welcher Kreatinin-Clearance ist die Einnahme von Metformin kontraindiziert?`\n",
    "\n",
    "<b style=\"color: gray;\">(max. achievable points: 8)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ae141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set language model and output parser\n",
    "def answer_query(query, k, index,texts):\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "query = \"Ab welcher Kreatinin-Clearance ist die Einnahme von Metformin kontraindiziert?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1a3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print result of test query with your chain (hint: input is a dictionary)\n",
    "print(answer_query(query, 4, index, chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34584dc",
   "metadata": {},
   "source": [
    "#### <b>Task (7): Implement a HyDE Query Transformation for RAG</b>\n",
    "<b>Task details:</b>\n",
    "- Implement a function that applies the HyDE strategy in your RAG pipeline.\n",
    "- add your HyDe transformation to your pipeline\n",
    "- Display the intermediate transformation (print statement within function is enough) and the final answer in the notebook.\n",
    "<b style=\"color: gray;\">(max. achievable points: 6)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query_hyde(query):\n",
    "    \n",
    "    return new_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bfb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_with_rewriting(query, k, index, texts):\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a8598",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Was ist der wichtigste Faktor bei der Diagnostizierung von Asthma?\"\n",
    "answer = answer_query_with_rewriting()\n",
    "print(\"LLM Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd20f4",
   "metadata": {},
   "source": [
    "#### <b>Task (7): Generate a list of test questions</b>\n",
    "<b>Task details:</b>\n",
    "- Create a Python list with 10 questions about the provided medications.\n",
    "- The questions should be automatically generated using a language model.\n",
    "- You may use chunks from the package inserts as inspiration, but this is not required.\n",
    "- At the end, print out your list of questions.\n",
    "\n",
    "<b style=\"color: gray;\">(max. achievable points: 6)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ca399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, question in enumerate(questions):\n",
    "    i +=1\n",
    "    print(\"Frage \" + str(1) + \": \" + question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e3225",
   "metadata": {},
   "source": [
    "#### <b>Task (8): Let your retriever answer the 10 generated questions.</b>\n",
    "<b>Task details:</b>\n",
    "- Use the 10 generated questions and have them answered by your RAG chain.\n",
    "- For each question, output both the retrieved documents and the answer.\n",
    "- Provide your own assessment of whether your chain works well or not.\n",
    "- Give an example of what worked well and what did not.\n",
    "\n",
    "<b style=\"color: gray;\">(max. achievable points: 6)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909478da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beantwortung der 10 generierten Fragen\n",
    "\n",
    "for question in questions:  # Questions list from Aufgabe (7)\n",
    "    \n",
    "    # Use the RAG chain to get an answer for the question\n",
    "    answer =  answer_query_with_rewriting(question, 4, index, chunks)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8e4d8",
   "metadata": {},
   "source": [
    "#### <b> TASK (9) Your assessment of the quality (double-click to edit the cell below):</b>\n",
    "\n",
    "- Briefly describe what seems to work well in your RAG pipeline based on the answers to the 10 generated questions above.\n",
    "- Give at least one example of a question/answer pair that worked particularly well.\n",
    "- Point out at least one aspect or example where the pipeline could be improved or did not work as expected.\n",
    "\n",
    "<b style=\"color: gray;\">(max. achievable points: 2)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc97c26",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff525b1e",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cae5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('IP Address:', socket.gethostbyname(socket.gethostname()))\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
